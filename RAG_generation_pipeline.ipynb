{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic RAG creation pipeline\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "from azure.search.documents.indexes.models import (\n",
        "    HnswAlgorithmConfiguration,\n",
        "    ExhaustiveKnnAlgorithmConfiguration,\n",
        "    VectorSearch,\n",
        "    VectorSearchProfile,\n",
        "    AzureOpenAIVectorizer,\n",
        "    AzureOpenAIVectorizerParameters,\n",
        "    SearchField,\n",
        "    SearchFieldDataType,\n",
        "    SearchIndex,\n",
        "    IndexProjectionMode,\n",
        "    SearchIndexerIndexProjection,\n",
        "    SearchIndexerIndexProjectionSelector,\n",
        "    SplitSkill,\n",
        "    InputFieldMappingEntry,\n",
        "    OutputFieldMappingEntry,\n",
        "    CognitiveServicesAccountKey,\n",
        "    SearchIndexerSkillset,\n",
        "    SearchIndexer,\n",
        "    SearchIndexerDataContainer,\n",
        "    SearchIndexerDataSourceConnection,\n",
        "    AzureOpenAIEmbeddingSkill,\n",
        "    SearchIndexerIndexProjectionsParameters\n",
        ")\n",
        "from azure.search.documents.indexes import SearchIndexerClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from azureml.core import Workspace\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import json\n",
        "\n",
        "#Load env variables\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "#Connect to ML workspace and MLClient\n",
        "subscription_id = config[\"AZML_SUBSCRIPTION_ID\"]\n",
        "resource_group = config[\"AZML_RESOURCE_GROUP\"]\n",
        "workspace_name = config[\"AZML_WORKSPACE_NAME\"]\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(),\n",
        "    subscription_id,\n",
        "    resource_group,\n",
        "    workspace_name,\n",
        ")\n",
        "\n",
        "#Set variables\n",
        "endpoint_name = config[\"AZML_ENDPOINT_NAME\"]\n",
        "endpoint = ml_client.online_endpoints.get(endpoint_name)\n",
        "credentials = ml_client.online_endpoints.get_keys(endpoint_name)\n",
        "\n",
        "cognitive_services_key = config[\"COGNITIVE_SERVICES_KEY\"]\n",
        "\n",
        "index_client = SearchIndexClient(endpoint=config[\"AI_SEARCH_SERVICE_ENDPOINT\"], credential=credential)\n",
        "indexer_client = SearchIndexerClient(\n",
        "    endpoint=config[\"AI_SEARCH_SERVICE_ENDPOINT\"],\n",
        "    credential=credential \n",
        ")\n",
        "\n",
        "data_source_name = config[\"DATA_SOURCE_NAME\"]\n",
        "container_name = config[\"CONTAINER_NAME\"]\n",
        "connection_string = config[\"STORAGE_CONNECTION_STRING\"]\n",
        "\n",
        "custom_embedder_endpoint = config[\"AZML_CUSTOM_MODEL_ENDPOINT\"]\n",
        "\n",
        "'''\n",
        "Set up a list of search algorithms and vectorizer techniques to use.\n",
        "A separate RAG model will be created for each possible combination of techniques.\n",
        "'''\n",
        "\n",
        "'''\n",
        "Specify openai service url and model names to use as embedder\n",
        "'''\n",
        "azure_openai_resource_url = config[\"AZURE_OPENAI_RESOURCE_URL\"]\n",
        "openai_embedder_deployment_names = [\"text-embedding-3-large\", \"text-embedding-ada-002-2\"]\n",
        "openai_embedder_model_names = [\"text-embedding-3-large\", \"text-embedding-ada-002\"]\n",
        "openai_embedding_dimensions = [1024, 1536]\n",
        "\n",
        "'''\n",
        "Specify online endpoints to use custom embedder\n",
        "'''\n",
        "endpoint_urls = [custom_embedder_endpoint]\n",
        "custom_embedding_dimensions = [384]\n",
        "\n",
        "'''\n",
        "Choose search algorithm(s)\n",
        "Azure currently supports: HNSW and ExhaustiveKNN.\n",
        "'''\n",
        "vector_search_algorithms = [\"HNSW\", \"ExhaustiveKNN\"]\n",
        "\n",
        "#Dynamically build vectorizers based on input lists of embedding models\n",
        "vectorizers = []\n",
        "if openai_embedder_deployment_names and openai_embedder_model_names and openai_embedding_dimensions:\n",
        "    for deployment_name, model_name, dimension in zip(openai_embedder_deployment_names, \n",
        "                                                    openai_embedder_model_names, \n",
        "                                                    openai_embedding_dimensions):\n",
        "        print(model_name)\n",
        "        vectorizers.append({\n",
        "            \"name\": f\"openai-vectorizer-{model_name}\".lower(),\n",
        "            \"kind\": \"azureOpenAI\",\n",
        "            \"vectorizer_params\": {\n",
        "                \"parameters\": AzureOpenAIVectorizerParameters(\n",
        "                    resource_url=azure_openai_resource_url,\n",
        "                    deployment_name=deployment_name,\n",
        "                    model_name=model_name,\n",
        "                ),\n",
        "            },\n",
        "            #Include deployment and model name explicitly to match the correct skillset later on\n",
        "            \"deployment_name\": deployment_name,\n",
        "            \"model_name\": model_name,\n",
        "            \"dimensions\": dimension,\n",
        "        })\n",
        "\n",
        "if endpoint_urls and custom_embedding_dimensions:\n",
        "    for endpoint_url, dimension in zip(endpoint_urls, custom_embedding_dimensions):\n",
        "        vectorizers.append({\n",
        "            \"name\": f\"custom-vectorizer-{dimension}\",\n",
        "            \"kind\": \"customWebApi\",\n",
        "            \"vectorizer_params\": {\n",
        "                \"customWebApiParameters\": {\n",
        "                    \"uri\": endpoint_url,\n",
        "                    \"httpMethod\": \"POST\",\n",
        "                    \"httpHeaders\": {\n",
        "                        \"Authorization\": f\"Bearer {credentials.primary_key}\",\n",
        "                    },\n",
        "                },\n",
        "            },\n",
        "            \"dimensions\": dimension,\n",
        "        })\n",
        "\n",
        "\n",
        "#Functions for pipeline execution\n",
        "def create_vector_search(algorithm_name, vectorizer_name, vectorizer_kind, vectorizer_params, vector_dimensions):\n",
        "    vectorizer = {\n",
        "        \"name\": vectorizer_name,\n",
        "        \"kind\": vectorizer_kind,\n",
        "        **vectorizer_params,\n",
        "    }\n",
        "    if algorithm_name == \"HNSW\":\n",
        "        algorithm_config = HnswAlgorithmConfiguration(name=\"myHnsw\")\n",
        "        profile_name = \"myHnswProfile\"\n",
        "    elif algorithm_name == \"ExhaustiveKNN\":\n",
        "        algorithm_config = ExhaustiveKnnAlgorithmConfiguration(\n",
        "            name=\"myKnn\",\n",
        "            parameters={\"metric\": \"euclidean\"}\n",
        "        )\n",
        "        profile_name = \"myExhaustiveKnnProfile\"\n",
        "\n",
        "    vector_search = VectorSearch(\n",
        "        algorithms=[algorithm_config],\n",
        "        profiles=[\n",
        "            VectorSearchProfile(\n",
        "                name=profile_name,\n",
        "                algorithm_configuration_name=algorithm_config.name,\n",
        "                vectorizer_name=vectorizer_name,\n",
        "            )\n",
        "        ],\n",
        "        vectorizers=[vectorizer],\n",
        "    )\n",
        "    return vector_search, profile_name\n",
        "\n",
        "\n",
        "def create_index(index_name, fields, vector_search):\n",
        "    '''\n",
        "    Creates the index that contains the embedded documents and enables retrieval\n",
        "    '''\n",
        "    index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)\n",
        "    return index\n",
        "\n",
        "\n",
        "def create_data_source_connection(name, connection_string, container_name):\n",
        "    '''\n",
        "    Data source connection\n",
        "    '''\n",
        "    container = SearchIndexerDataContainer(name=container_name)\n",
        "    data_source_connection = SearchIndexerDataSourceConnection(\n",
        "        name=name,\n",
        "        type=\"azureblob\",\n",
        "        connection_string=connection_string,\n",
        "        container=container\n",
        "    )\n",
        "    return data_source_connection\n",
        "\n",
        "def create_skillset(skillset_name, index_projections, cognitive_services_key, vectorizer_kind, vectorizer_context=None, vectorizer_uri=None, vectorizer_headers=None, vector_dimensions=None, deployment_name=None, model_name=None):\n",
        "    '''\n",
        "    Skillset creation\n",
        "    Creats split skill and embedding skill\n",
        "    Embedding skill is based on the vectorizer configuration as it's necessary that they match\n",
        "    '''\n",
        "    #Split skill\n",
        "    split_skill = SplitSkill(\n",
        "        description=\"Split skill to chunk documents\",\n",
        "        text_split_mode=\"pages\",\n",
        "        context=\"/document\",\n",
        "        maximum_page_length=2000,\n",
        "        page_overlap_length=500,\n",
        "        inputs=[\n",
        "            InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),\n",
        "        ],\n",
        "        outputs=[\n",
        "            OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\"),\n",
        "        ],\n",
        "    )\n",
        "    #Embedding skill\n",
        "    if vectorizer_kind == \"customWebApi\":\n",
        "        embedding_skill = {\n",
        "            \"@odata.type\": \"#Microsoft.Skills.Custom.WebApiSkill\",\n",
        "            \"name\": \"customEmbeddingSkill\",\n",
        "            \"description\": f\"Custom embedder from ML endpoint ({vector_dimensions})\",\n",
        "            \"uri\": vectorizer_uri,\n",
        "            \"context\": vectorizer_context,\n",
        "            \"timeout\": \"PT30S\",\n",
        "            \"batchSize\": 1,\n",
        "            \"httpMethod\": \"POST\",\n",
        "            \"httpHeaders\": vectorizer_headers,\n",
        "            \"inputs\": [\n",
        "                InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),\n",
        "            ],\n",
        "            \"outputs\": [\n",
        "                OutputFieldMappingEntry(name=\"vector\", target_name=\"text_vector\"),\n",
        "            ],\n",
        "        }\n",
        "    else:\n",
        "        embedding_skill = AzureOpenAIEmbeddingSkill(\n",
        "            description=\"Skill to generate embeddings via Azure OpenAI\",\n",
        "            context=\"/document/pages/*\",\n",
        "            resource_url=azure_openai_resource_url,\n",
        "            deployment_name=deployment_name,\n",
        "            model_name=model_name,\n",
        "            dimensions=vector_dimensions,\n",
        "            inputs=[\n",
        "                InputFieldMappingEntry(name=\"text\", source=\"/document/pages/*\"),\n",
        "            ],\n",
        "            outputs=[\n",
        "                OutputFieldMappingEntry(name=\"embedding\", target_name=\"text_vector\"),\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    #Create skillset\n",
        "    skills = [split_skill, embedding_skill]\n",
        "    skillset = SearchIndexerSkillset(\n",
        "        name=skillset_name,\n",
        "        description=\"RAG skillset with selected techniques\",\n",
        "        skills=skills,\n",
        "        index_projection=index_projections,\n",
        "        cognitive_services_account=CognitiveServicesAccountKey(key=cognitive_services_key),\n",
        "    )\n",
        "    return skillset\n",
        "\n",
        "\n",
        "def create_indexer(indexer_name, skillset_name, target_index_name, data_source_name):\n",
        "    '''\n",
        "    Creates the indexer that fills the index with with embedded documents using the data source and embedding skill\n",
        "    '''\n",
        "    indexer = SearchIndexer(\n",
        "        name=indexer_name,\n",
        "        description=f\"Indexer using skillset {skillset_name} for index {target_index_name}\",\n",
        "        skillset_name=skillset_name,\n",
        "        target_index_name=target_index_name,\n",
        "        data_source_name=data_source_name,\n",
        "    )\n",
        "    return indexer\n",
        "\n",
        "\n",
        "#Main pipeline execution loop\n",
        "for algorithm, vectorizer in product(vector_search_algorithms, vectorizers):\n",
        "    vectorizer_name = vectorizer[\"name\"]\n",
        "    vectorizer_kind = vectorizer[\"kind\"]\n",
        "    vector_dimensions = vectorizer[\"dimensions\"]\n",
        "\n",
        "    #Get the same models as the vectorizer to use in the embedding skill\n",
        "    deployment_name = None\n",
        "    model_name = None\n",
        "    if vectorizer_kind == \"azureOpenAI\":\n",
        "        deployment_name = vectorizer[\"deployment_name\"]\n",
        "        model_name = vectorizer[\"model_name\"]\n",
        "\n",
        "    #Generate names\n",
        "    index_name = f\"idx-{algorithm.lower()}-{vectorizer_name.replace('_', '-')}\".strip('-')[:128]\n",
        "    skillset_name = f\"ss-{index_name}\"[:128]\n",
        "    indexer_name = f\"idxr-{index_name}\"[:128]\n",
        "\n",
        "    vector_search, vector_search_profile_name = create_vector_search(\n",
        "        algorithm_name=algorithm,\n",
        "        vectorizer_name=vectorizer_name,\n",
        "        vectorizer_kind=vectorizer_kind,\n",
        "        vectorizer_params=vectorizer.get(\"vectorizer_params\", {}),\n",
        "        vector_dimensions=vector_dimensions,\n",
        "    )\n",
        "\n",
        "    #Define fields using vector_search_profile_name\n",
        "    fields = [\n",
        "        SearchField(name=\"parent_id\", type=SearchFieldDataType.String),\n",
        "        SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
        "        SearchField(\n",
        "            name=\"chunk_id\",\n",
        "            type=SearchFieldDataType.String,\n",
        "            key=True,\n",
        "            sortable=True,\n",
        "            filterable=True,\n",
        "            facetable=True,\n",
        "            analyzer_name=\"keyword\",\n",
        "        ),\n",
        "        SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
        "        SearchField(\n",
        "            name=\"text_vector\",\n",
        "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
        "            vector_search_dimensions=vector_dimensions,\n",
        "            vector_search_profile_name=vector_search_profile_name,\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    #Create index\n",
        "    index = create_index(index_name=index_name, fields=fields, vector_search=vector_search)\n",
        "\n",
        "    try:\n",
        "        index_client.create_or_update_index(index)\n",
        "        print(f\"Index '{index_name}' created or updated successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create index '{index_name}': {e}\")\n",
        "\n",
        "    #Create index projections\n",
        "    index_projections = SearchIndexerIndexProjection(selectors=[\n",
        "        SearchIndexerIndexProjectionSelector(\n",
        "            target_index_name=index_name,\n",
        "            parent_key_field_name=\"parent_id\",\n",
        "            source_context=\"/document/pages/*\",\n",
        "            mappings=[\n",
        "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/pages/*\"),\n",
        "                InputFieldMappingEntry(name=\"text_vector\", source=\"/document/pages/*/text_vector\"),\n",
        "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),\n",
        "            ],\n",
        "        )\n",
        "    ], parameters=SearchIndexerIndexProjectionsParameters(\n",
        "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS\n",
        "    ))\n",
        "\n",
        "    #Create skillset\n",
        "    skillset = create_skillset(\n",
        "        skillset_name=skillset_name,\n",
        "        index_projections=index_projections,\n",
        "        cognitive_services_key=cognitive_services_key,\n",
        "        vectorizer_kind=vectorizer_kind,\n",
        "        vectorizer_context=\"/document/pages/*\" if vectorizer_kind == \"customWebApi\" else None,\n",
        "        vectorizer_uri=vectorizer[\"vectorizer_params\"][\"customWebApiParameters\"][\"uri\"] if vectorizer_kind == \"customWebApi\" else None,\n",
        "        vectorizer_headers={\"Authorization\": f\"Bearer {credentials.primary_key}\"} if vectorizer_kind == \"customWebApi\" else None,\n",
        "        vector_dimensions=vector_dimensions,\n",
        "        deployment_name=deployment_name,\n",
        "        model_name=model_name,\n",
        "    )\n",
        "\n",
        "    indexer_client.create_or_update_skillset(skillset)\n",
        "    print(f\"Skillset '{skillset_name}' created.\")\n",
        "\n",
        "    #Create data source connection\n",
        "    data_source_connection = create_data_source_connection(\n",
        "        name=data_source_name,\n",
        "        connection_string=connection_string,\n",
        "        container_name=container_name,\n",
        "    )\n",
        "    data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
        "\n",
        "    #Create indexer\n",
        "    indexer = create_indexer(\n",
        "        indexer_name=indexer_name,\n",
        "        skillset_name=skillset_name,\n",
        "        target_index_name=index_name,\n",
        "        data_source_name=data_source.name,\n",
        "    )\n",
        "    indexer_client.create_or_update_indexer(indexer)\n",
        "    print(f\"Indexer '{indexer_name}' created and running. Allow a few minutes for indexing.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Overriding of current TracerProvider is not allowed\nOverriding of current LoggerProvider is not allowed\nOverriding of current MeterProvider is not allowed\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\nAttempting to instrument while already instrumented\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "text-embedding-3-large\ntext-embedding-ada-002\nIndex 'idx-hnsw-openai-vectorizer-text-embedding-3-large' created or updated successfully.\nSkillset 'ss-idx-hnsw-openai-vectorizer-text-embedding-3-large' created.\nIndexer 'idxr-idx-hnsw-openai-vectorizer-text-embedding-3-large' created and running. Allow a few minutes for indexing.\nIndex 'idx-hnsw-openai-vectorizer-text-embedding-ada-002' created or updated successfully.\nSkillset 'ss-idx-hnsw-openai-vectorizer-text-embedding-ada-002' created.\nIndexer 'idxr-idx-hnsw-openai-vectorizer-text-embedding-ada-002' created and running. Allow a few minutes for indexing.\nIndex 'idx-hnsw-custom-vectorizer-384' created or updated successfully.\nSkillset 'ss-idx-hnsw-custom-vectorizer-384' created.\nIndexer 'idxr-idx-hnsw-custom-vectorizer-384' created and running. Allow a few minutes for indexing.\nIndex 'idx-exhaustiveknn-openai-vectorizer-text-embedding-3-large' created or updated successfully.\nSkillset 'ss-idx-exhaustiveknn-openai-vectorizer-text-embedding-3-large' created.\nIndexer 'idxr-idx-exhaustiveknn-openai-vectorizer-text-embedding-3-large' created and running. Allow a few minutes for indexing.\nIndex 'idx-exhaustiveknn-openai-vectorizer-text-embedding-ada-002' created or updated successfully.\nSkillset 'ss-idx-exhaustiveknn-openai-vectorizer-text-embedding-ada-002' created.\nIndexer 'idxr-idx-exhaustiveknn-openai-vectorizer-text-embedding-ada-002' created and running. Allow a few minutes for indexing.\nIndex 'idx-exhaustiveknn-custom-vectorizer-384' created or updated successfully.\nSkillset 'ss-idx-exhaustiveknn-custom-vectorizer-384' created.\nIndexer 'idxr-idx-exhaustiveknn-custom-vectorizer-384' created and running. Allow a few minutes for indexing.\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1753114830968
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.10 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}