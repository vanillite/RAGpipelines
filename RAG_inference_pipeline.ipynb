{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### RAG inferencing pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of user queries to handle with agentic retrieval:\n",
        "1. No question asked -> LLM response\n",
        "2. Single and simple question about data -> Rewrite -> Embed -> Rerank -> LLM response\n",
        "3. Complex question about data -> Expand question into sub questions -> Embed -> Reciprocal rank fusion -> LLM response\n",
        "4. Multiple questions about data -> Sub-query decomposition -> Embed -> Rerank -> LLM response"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM prompts:\n",
        "- Classification\n",
        "- Query rewriting\n",
        "- Query rewriting + query expansion\n",
        "- Query rewriting + sub-query decomposition\n",
        "- Reranking\n",
        "- Reciprocal Rank Fusion\n",
        "- Regular response generation\n",
        "- Response generation with retrieved documents"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.identity import get_bearer_token_provider\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.models import VectorizableTextQuery\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from openai import AzureOpenAI\n",
        "from pydantic import BaseModel, conint, conlist \n",
        "import json\n",
        "from operator import itemgetter\n",
        "import time\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1752011688859
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables\n",
        "service_endpoint = \"\"\n",
        "# index_name = \"idx-exhaustiveknn-openai-vectorizer-text-embedding-3-large\"\n",
        "index_name = \"\"\n",
        "key = \"\"\n",
        "\n",
        "#max number of sub queries that can be generated from a single complex user query\n",
        "max_num_queries_complex = 3\n",
        "\n",
        "#max number of sub queries that can be generated from a single user query that contains multiple questions\n",
        "max_num_queries_multiple = 5\n",
        "\n",
        "#Number of relevant documents being returned by the search algorithm\n",
        "nr_search = 100\n",
        "\n",
        "#Number of relevant documents the reranker returns to the LLM\n",
        "nr_docs = 20\n",
        "\n",
        "#Number of relevant documents the RRF algorithm returns to the LLM\n",
        "nr_rrf_docs = 20\n",
        "\n",
        "\n",
        "#Configuration of retrieval methods\n",
        "#Each tuple contains (method_name, use_agentic, use_reranking)\n",
        "selected_methods = [\n",
        "    (\"Non-agentic retrieval\", False, False),\n",
        "    (\"Agentic retrieval without reranking\", True, False),\n",
        "    (\"Agentic retrieval with reranking\", True, True)\n",
        "]\n",
        "\n",
        "#List of all index names from Azure AI search to use\n",
        "all_index_names = [\n",
        "    \"idx-exhaustiveknn-custom-vectorizer-384\",\n",
        "    \"idx-exhaustiveknn-openai-vectorizer-text-embedding-3-large\",\n",
        "    \"idx-exhaustiveknn-openai-vectorizer-text-embedding-ada-002\",\n",
        "    \"idx-hnsw-custom-vectorizer-384\",\n",
        "    \"idx-hnsw-openai-vectorizer-text-embedding-3-large\",\n",
        "    \"idx-hnsw-openai-vectorizer-text-embedding-ada-002\"\n",
        "]\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1752011689152
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1752006599329
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"\")\n",
        "\n",
        "#Model selection\n",
        "client = AzureOpenAI(\n",
        "     api_version=\"2024-12-01-preview\",\n",
        "     azure_endpoint=\"\",\n",
        "     azure_ad_token_provider=token_provider\n",
        " )\n",
        "\n",
        "deployment_names = [\"gpt-4o\", \"gpt-4.1\"]\n",
        "\n",
        "#Set up the AI Search client\n",
        "search_client = SearchClient(\n",
        "     endpoint=service_endpoint,\n",
        "     index_name=index_name,\n",
        "     credential=AzureKeyCredential(key)\n",
        " )\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1752011691244
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSIFICATION_PROMPT=\"\"\"\n",
        "You are an AI assistant that help users retrieve information from the source material.\n",
        "The source material for answering the questions is the \"Handboek Financiële Informatie en Administratie Rijksoverheid\" manual, which is in Dutch. Below is a description of its content:\n",
        "###\n",
        "De verschillende bronnen beschrijven *diverse aspecten van het Nederlandse overheidsbeleid en financieel beheer, waaronder **begrotingsprocessen, de **rol van verschillende overheidsinstanties* en *specifieke richtlijnen voor subsidieverlening en projectmanagement. Ze behandelen de **structuur en verantwoording van overheidsfinanciën, de **evaluatie van beleid op doeltreffendheid en doelmatigheid, en **protocollen voor grote projecten en het schatkistbankieren. De teksten belichten ook de **principes van duurzame ontwikkeling en brede welvaart* in relatie tot beleidsvorming, evenals *juridische kaders en beginselen* die de uitvoering van overheidsbeleid sturen. Tenslotte wordt aandacht besteed aan de *communicatie van overheidsinformatie* en de *betrokkenheid van belanghebbenden* bij beleidsprocessen.\n",
        "###\n",
        "\n",
        "Instructions: \n",
        "###\n",
        "You are given the user query in Dutch.\n",
        "This query should either be used for document retrieval, or it should warrent a regular response without retrieval.\n",
        "Your task is to classify the user query into one of the four categories. \n",
        "You are responsible for selecting the correct category such that a specialized downstream task can be applied correctly.\n",
        "Return ONLY the number 1, 2, 3, or 4, that corresponds to the correct category.\n",
        "Do not include any other details in your response, only provide a single number.\n",
        "\n",
        "Choose which of the four category the query belongs to:\n",
        "1: The query is completely unrelated to \"Het Handboek Financiële Informatie en Administratie Rijksoverheid\"\n",
        "2: The query contains a single simple question that can directly be answered through embedding and document search.\n",
        "3: the query contains a single complex question that warrents the query to be expanded first, before applying embedding and document search.\n",
        "4. The query contains multiple questions that need to be split up such that they can be embedded and answered separately. \n",
        "\n",
        "Examples:\n",
        "---\n",
        "Example_query_0: \"Goedendag\"\n",
        "Reponse: 1\n",
        "\n",
        "Example_query_1: \"Welke kleuren heeft de regenboog?\"\n",
        "Response: 1\n",
        "\n",
        "Example_query_2: \"Welke departementen bevinden zich binnen het Ministerie van Financiën?\"\n",
        "Reponse: 2\n",
        "\n",
        "Example_query_3: \"Hebben jullie een procesbeschrijving van de controle van begrotingswetten en publicatie in de Staatscourant?\n",
        "Response: 3\n",
        "\n",
        "Example_query_4: \"Hoe kan de overheid kosten besparen, en welke praktische stappen zijn nodig om dit te bereiken? Hoe wordt de rol van inspecteurs in dit proces bepaald?\"\n",
        "Reponse: 4\n",
        "---\n",
        "###\n",
        "\n",
        "User query:\n",
        "### \n",
        "{query} \n",
        "###\n",
        "\n",
        "Response:\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1752011691832
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Structured LLM output for query classification\n",
        "#Restricts the output to be an int between 1 and 4\n",
        "class ClassificationEvent(BaseModel):\n",
        "    classification: conint(ge=1, le=4)\n",
        "\n",
        "def classify_query(query, deployment_name):\n",
        "    \"\"\"\n",
        "    Classifies the user query using the classification prompt.\n",
        "    Returns thee classification result for the query.\n",
        "    \"\"\"\n",
        "    query_classification_response = client.beta.chat.completions.parse(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": CLASSIFICATION_PROMPT.format(query=query)\n",
        "            }\n",
        "        ],\n",
        "        model=deployment_name,\n",
        "        response_format=ClassificationEvent\n",
        "    )\n",
        "    \n",
        "    # Extract the classification from the response\n",
        "    classification = json.loads(query_classification_response.choices[0].message.content)[\"classification\"]\n",
        "    return classification"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1752011692140
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONVERSATIONAL_PROMPT=\"\"\"\n",
        "You are a helpful AI assistant. Assist the user with any questions or tasks to the best of your ability.\n",
        "\n",
        "User query:\n",
        "###\n",
        "{query}\n",
        "###\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1752011692827
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SIMPLE_PROMPT=\"\"\"\n",
        "You are an expert in refining a simple user question into a clear and searchable query. \n",
        "\n",
        "Instructions:  \n",
        "* Analyze the user's query and identify ways to improve clarity and flow while preserving its meaning.  \n",
        "* Do NOT add new information or alter the query's original intent.  \n",
        "* If the query contains ambiguous wording, aim to structure it more clearly and with greater precision.  \n",
        "* Maintain the tone and style of the original question.\n",
        "* If there are acronyms or words you are not familiar with, do not rephrase them.\n",
        "* Generate the query in the same language as the orignal user query.\n",
        "\n",
        "Here are some examples:  \n",
        "###  \n",
        "Example_user_query: \"Hoe omgaan belastingplichtigen die typisch met bijzondere situaties?\"  \n",
        "Query: \"Hoe gaan belastingplichtigen om met bijzondere situaties?\"  \n",
        "\n",
        "Example_user_query: \"Welk stappen moet ik doorlopen voor overheid financien precies gecontroleed?\"  \n",
        "Query: \"Welke stappen moeten worden doorlopen om overheidsfinanciën precies te controleren?\"  \n",
        "###  \n",
        "\n",
        "User query:\n",
        "### \n",
        "{query} \n",
        "###\n",
        "\n",
        "Query:\n",
        "\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1752011693145
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Structured LLM output for query rewriting\n",
        "#Constraints the output to be a single string\n",
        "class SimpleQueryEvent(BaseModel):\n",
        "    queries: str\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1752011693416
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COMPLEX_PROMPT=\"\"\"\n",
        "You are an expert in expanding complex user questions into actionable sub-queries. Your expertise lies in analyzing complex questions, breaking them down into multiple smaller sub-queries that together answer the original question effectively. \n",
        "\n",
        "The source material for answering the questions is the \"Handboek Financiële Informatie en Administratie Rijksoverheid\" manual, which is in Dutch. Below is a description of its content:\n",
        "###\n",
        "De verschillende bronnen beschrijven *diverse aspecten van het Nederlandse overheidsbeleid en financieel beheer, waaronder **begrotingsprocessen, de **rol van verschillende overheidsinstanties* en *specifieke richtlijnen voor subsidieverlening en projectmanagement. Ze behandelen de **structuur en verantwoording van overheidsfinanciën, de **evaluatie van beleid op doeltreffendheid en doelmatigheid, en **protocollen voor grote projecten en het schatkistbankieren. De teksten belichten ook de **principes van duurzame ontwikkeling en brede welvaart* in relatie tot beleidsvorming, evenals *juridische kaders en beginselen* die de uitvoering van overheidsbeleid sturen. Tenslotte wordt aandacht besteed aan de *communicatie van overheidsinformatie* en de *betrokkenheid van belanghebbenden* bij beleidsprocessen.\n",
        "###\n",
        "\n",
        "Instructions:\n",
        "* Analyze the complex user query and understand its core components.\n",
        "* These sub-queries, when answered, should collectively help address the original user query.\n",
        "* Generate a maximum of {max_num_queries_complex} sub-queries, one on each line. \n",
        "* Do not include any other details in your response, only provide the queries.\n",
        "\n",
        "These are the guidelines you consider when completing your task:\n",
        "* Carefully analyze the user query and understand the core question.\n",
        "* The sub queries should be relevant to the user query.\n",
        "* Identify different aspects, contexts, or components required to answer the original query.\n",
        "* Generate clear, specific, and actionable sub-queries that will assist in gathering relevant information for answering the original query.\n",
        "* If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
        "* Generate the sub-queries in the same language as the orignal user query.\n",
        "\n",
        "Example: \n",
        "###\n",
        "Example_user_query: \"Hoe worden overheidsfinanciën gecontroleerd en beheerd, inclusief budgettering en verantwoording?\"\n",
        "Queries: Hoe worden overheidsfinanciën beheerd volgens comptabele regelgeving?  \n",
        "Welke voorschriften gelden binnen de rijksoverheid voor budgettering en financieel beheer?  \n",
        "Wat zijn de regels en procedures voor financiële controle en verantwoording binnen de rijksoverheid?  \n",
        "###\n",
        "\n",
        "User query:\n",
        "### \n",
        "{query} \n",
        "###\n",
        "\n",
        "Queries:\n",
        "\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1752011693638
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Structured LLM output for query expansion\n",
        "#Constraints the output to be a list of strings, ranging from 1 to n objects\n",
        "class ComplexQueryEvent(BaseModel):\n",
        "    queries: conlist(str, min_length = 1, max_length = max_num_queries_complex)\n"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1752011693888
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MULTIPLE_PROMPT=\"\"\"\n",
        "You are an expert in analyzing and organizing user queries that contain multiple questions into separate questions.\n",
        "You will receive a user query that contains multiple questions.\n",
        "Your task is to separate the user query into distinct and clear individual queries that can be used for document search.\n",
        "\n",
        "Instructions:  \n",
        "* Carefully analyze the user query and identify each distinct question it contains.  \n",
        "* Decompose the query into separate, logically ordered queries, ensuring each one is complete and actionable on its own.  \n",
        "* Maintain the original meaning and intent of each question.\n",
        "* Only generate as many questions as the original user query contains.\n",
        "* Do not include any other details in your response, only provide the queries.\n",
        "* Generate a maximum of {max_num_queries_multiple} queries, one on each line.\n",
        "\n",
        "These are the guidelines you consider when completing your task:\n",
        "* Write each query clearly and concisely, one per line.\n",
        "* Generate clear, specific, and actionable sub-queries that will assist in gathering relevant information for answering the original query.\n",
        "* You may fix typos and improve clarity.\n",
        "* If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
        "* Ensure no question is omitted or altered in meaning.\n",
        "* Generate the queries in the same language as the orignal user query.\n",
        "\n",
        "Example: \n",
        "###\n",
        "Example_query_1: \"Hoe worden overheidsfinanciën gecontroleerd en beheerd? Hoe verschilt dit van budgettering in private organisaties?\"\n",
        "Queries: Hoe worden overheidsfinanciën gecontroleerd?  \n",
        "Hoe worden overheidsfinanciën beheerd?  \n",
        "Hoe verschilt budgettering in private organisaties van budgettering binnen de overheid? \n",
        "###\n",
        "\n",
        "Query:\n",
        "### \n",
        "{query}\n",
        "###\n",
        "\n",
        "Queries:\n",
        "\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1752011694127
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Structured LLM output for query decomposition\n",
        "#Constraints the output to be a list of strings, ranging from 1 to n objects\n",
        "class MultipleQueryEvent(BaseModel):\n",
        "    queries: conlist(str, min_length = 1, max_length = max_num_queries_multiple)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1752011694607
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grounded_prompt_rewriting=\"\"\"\n",
        "You are an AI assistant that helps users learn from the information found in the source material.\n",
        "Answer the query using only the sources provided below.\n",
        "You will be given the original query from the user, as well as the transformed query that is used for optimized searching.\n",
        "Answer with the facts listed in the list of sources below. \n",
        "Cite your source when you answer the question.\n",
        "If there isn't enough information below, say that the sources don't provide an answer to the question.\n",
        "Answer the user in Dutch.\n",
        "\n",
        "Original query: \n",
        "###\n",
        "{original_query}\n",
        "###\n",
        "\n",
        "Transformed query: \n",
        "###\n",
        "{transformed_query}\n",
        "###\n",
        "\n",
        "Sources:\n",
        "###\n",
        "{sources}\n",
        "###\n",
        "\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1752011694873
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grounded_prompt_expansion=\"\"\"\n",
        "You are an AI assistant that helps users learn from the information found in the source material.\n",
        "Answer the query using only the sources provided below.\n",
        "You will be given the original query from the user, as well as the expanded sub queries that were used for optimized searching.\n",
        "Answer with the facts listed in the list of sources below. \n",
        "Cite your source when you answer the question.\n",
        "If there isn't enough information below, say that the sources don't provide an answer to the question.\n",
        "Answer the user in Dutch.\n",
        "\n",
        "Original query: \n",
        "###\n",
        "{original_query}\n",
        "###\n",
        "\n",
        "Expanded queries: \n",
        "###\n",
        "{transformed_query}\n",
        "###\n",
        "\n",
        "Sources:\n",
        "###\n",
        "{sources}\n",
        "###\n",
        "\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1752011695143
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grounded_prompt_decomposition=\"\"\"\n",
        "You are an AI assistant that helps users learn from the information found in the source material.\n",
        "The user query contains multiple questions.\n",
        "An LLM has decomposed the query, extracting each question separately for optimized searching.\n",
        "You will answer each query using their corresponding sources\n",
        "Answer each query using the corresponding sources.\n",
        "Answer with the facts listed in the list of sources below. \n",
        "Cite your source when you answer the question.\n",
        "If there isn't enough information below, say that the sources don't provide an answer to the question.\n",
        "Answer the user in Dutch.\n",
        "\n",
        "Original query: \n",
        "###\n",
        "{original_query}\n",
        "###\n",
        "\n",
        "Decomposed queries: \n",
        "###\n",
        "{transformed_query}\n",
        "###\n",
        "\n",
        "Sources:\n",
        "###\n",
        "{sources}\n",
        "###\n",
        "\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1752011695388
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grounded_prompt_non_agentic=\"\"\"\n",
        "You are an AI assistant that helps users learn from the information found in the source material.\n",
        "Answer the query using only the sources provided below.\n",
        "You will be given the original query from the user, as well as the transformed query that is used for optimized searching.\n",
        "Answer with the facts listed in the list of sources below. \n",
        "Cite your source when you answer the question.\n",
        "If there isn't enough information below, say that the sources don't provide an answer to the question.\n",
        "Answer the user in Dutch.\n",
        "\n",
        "Original query: \n",
        "###\n",
        "{original_query}\n",
        "###\n",
        "\n",
        "Sources:\n",
        "###\n",
        "{sources}\n",
        "###\n",
        "\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1752011695620
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "#Loads reranker into memory, takes +- 2 minutes\n",
        "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n2025-07-08 21:56:10.015362: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752011770.779153   30349 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752011770.996701   30349 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1752011773.019012   30349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1752011773.019036   30349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1752011773.019039   30349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1752011773.019041   30349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-07-08 21:56:13.240874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1752011820324
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search Client invoke functions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PipelineConfig:\n",
        "    \"\"\"\n",
        "    Configuration for the retrieval pipeline.\n",
        "    \"\"\"\n",
        "    def __init__(self, use_agentic_retrieval: bool = True, use_reranking: bool = True):\n",
        "        self.use_agentic_retrieval = use_agentic_retrieval\n",
        "        self.use_reranking = use_reranking"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1752012205764
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_search_results(search_results):\n",
        "    '''\n",
        "    Parses search_results information for each subquery for RRF\n",
        "    '''\n",
        "    chunk_dict = {}\n",
        "    result_dict = {}\n",
        "    for query, query_result in search_results.items():\n",
        "        temp_dict = {}\n",
        "        # print(query)\n",
        "        for result in query_result:\n",
        "            chunk_dict[result['chunk_id']] = result\n",
        "            temp_dict[result['chunk_id']] = result['@search.score']\n",
        "        result_dict[query] = temp_dict\n",
        "    #result_dict contains the query as keys with chunk_id and score as values\n",
        "    #chunk_dict contains all the info\n",
        "    return result_dict, chunk_dict\n",
        "\n",
        "\n",
        "\n",
        "#RRF implementation from https://github.com/Raudaschl/rag-fusion/blob/master/main.py\n",
        "def reciprocal_rank_fusion(search_results_dict, k=60):\n",
        "    fused_scores = {}\n",
        "    for query, doc_scores in search_results_dict.items():\n",
        "        for rank, (doc, score) in enumerate(sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)):\n",
        "            if doc not in fused_scores:\n",
        "                fused_scores[doc] = 0\n",
        "            previous_score = fused_scores[doc]\n",
        "            fused_scores[doc] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = {doc: score for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)}\n",
        "    return reranked_results"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1752012207046
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_conversational(query, deployment_name):\n",
        "    '''\n",
        "    Directly generate a response to the user query without retrieval\n",
        "    '''\n",
        "    query_conversational_response = client.beta.chat.completions.parse(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": CONVERSATIONAL_PROMPT.format(query=query)\n",
        "        }\n",
        "    ],\n",
        "    model=deployment_name,\n",
        "    )\n",
        "    return query_conversational_response.choices[0].message.content\n",
        "\n",
        "\n",
        "def search_simple_with_optional_ranking(query, rerank, search_client, deployment_name):\n",
        "    '''\n",
        "    Execute simple search with query rewriting and reranking\n",
        "    '''\n",
        "    query_simple_response = client.beta.chat.completions.parse(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": SIMPLE_PROMPT.format(query=query)\n",
        "            }\n",
        "        ],\n",
        "        model=deployment_name,\n",
        "        response_format=SimpleQueryEvent\n",
        "    )\n",
        "    #Rewritten user query\n",
        "    query_rewriting = json.loads(query_simple_response.choices[0].message.content)[\"queries\"]\n",
        "\n",
        "    #Configure vectorized query\n",
        "    vector_query = VectorizableTextQuery(text=query_rewriting, k_nearest_neighbors=nr_search, fields=\"text_vector\")\n",
        "\n",
        "    #Perform vector search on the single query\n",
        "    #Search_results is an iterator object containing the relevant documents\n",
        "    #Convert result to list to prevent exhausting the iterator\n",
        "    search_results = list(search_client.search(\n",
        "        search_text=query_rewriting,\n",
        "        vector_queries=[vector_query],\n",
        "        select=[\"title\", \"chunk\"],\n",
        "        top=nr_search,\n",
        "    ))\n",
        "\n",
        "    #Use reranking or not based on configurations\n",
        "    if rerank:\n",
        "        #Use reranker to rerank the search results\n",
        "        reranker_input = [[query_rewriting, document['chunk']] for document in search_results]\n",
        "        scores = reranker.compute_score(reranker_input, normalize=True)\n",
        "\n",
        "        ranked_documents = [\n",
        "            {\n",
        "                \"title\": document[\"title\"],\n",
        "                \"chunk\": document[\"chunk\"],\n",
        "                \"score\": score,\n",
        "            }\n",
        "            for document, score in zip(search_results, scores)\n",
        "        ]\n",
        "\n",
        "        #Sort documents by highest relevancy score and return top nr_docs documents\n",
        "        top_documents = sorted(ranked_documents, key=itemgetter(\"score\"), reverse=True)[:nr_docs]\n",
        "    else:\n",
        "        #Directly return the top nr_docs documents without reranking\n",
        "        top_documents = search_results[:nr_docs]\n",
        "\n",
        "\n",
        "    #Extract chunks of the top_documents to export to df and use for evaluation\n",
        "    top_chunks = [doc[\"chunk\"] for doc in top_documents]\n",
        "    #print(top_chunks)\n",
        "\n",
        "    #Format documents for LLM readability\n",
        "    sources_formatted_simple = \"=================\\n\".join(\n",
        "        [f'TITLE: {doc[\"title\"]}, CONTENT: {doc[\"chunk\"]}' for doc in top_documents]\n",
        "    )\n",
        "\n",
        "    #Generate LLM response using the relevant documents\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": grounded_prompt_rewriting.format(\n",
        "                    original_query=query,\n",
        "                    transformed_query=query_rewriting,\n",
        "                    sources=sources_formatted_simple,\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        model=deployment_name\n",
        "    )\n",
        "    return response.choices[0].message.content, top_chunks\n",
        "\n",
        "\n",
        "def search_complex_with_optional_ranking(query, rerank, search_client, deployment_name):\n",
        "    '''\n",
        "    Execute complex search with query expansion and (optional) reciprocal rank fusion (RRF)\n",
        "    '''\n",
        "    query_complex_response = client.beta.chat.completions.parse(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": COMPLEX_PROMPT.format(query=query, max_num_queries_complex=max_num_queries_complex)\n",
        "            }\n",
        "        ],\n",
        "        model=deployment_name,\n",
        "        response_format=ComplexQueryEvent\n",
        "    )\n",
        "\n",
        "    #Contains list of subqueries expanded from the original user query\n",
        "    query_expansion = json.loads(query_complex_response.choices[0].message.content)[\"queries\"]\n",
        "\n",
        "    #print(f\"query_expansion: {query_expansion}\")\n",
        "    search_results = {}\n",
        "    \n",
        "    #Perform vector search separately for each subquery\n",
        "    #Store all results for all subqueries in search_results\n",
        "    for subquery in query_expansion:\n",
        "        vector_query = VectorizableTextQuery(text=subquery, k_nearest_neighbors=nr_search, fields=\"text_vector\")\n",
        "        search_results[subquery] = list(search_client.search(\n",
        "            search_text=subquery,\n",
        "            vector_queries=[vector_query],\n",
        "            select=[\"title\", \"chunk\", \"chunk_id\"],\n",
        "            top=nr_search,\n",
        "        ))\n",
        "\n",
        "    #print(f\"length search_results: {len(search_results)}\")\n",
        "\n",
        "    #Process results with or without RRF\n",
        "    if rerank:\n",
        "        #Using search_results, create two dictionaries.\n",
        "        #result_dict contains the query as keys with chunk_id and score as values\n",
        "        #chunk_dict contains all the info from search_results\n",
        "        result_dict, chunk_dict = process_search_results(search_results)\n",
        "\n",
        "        #rrf_scores is a dictionary containing the ranking of all the documents from all subqueries fused together.\n",
        "        #It usually contains less documents than search_results has since it fuses together duplicate queries.\n",
        "        #The documents are sorted in descending order with the highest similarity scoring documents at the beginning\n",
        "        #Take the top nr_rrf_docs documents to give to the LLM \n",
        "        rrf_scores = reciprocal_rank_fusion(result_dict)\n",
        "        #print(f\"length rrf_scores: {len(rrf_scores)}\")\n",
        "        top_documents = [\n",
        "            {\n",
        "                \"title\": chunk_dict[chunk_id][\"title\"],\n",
        "                \"chunk\": chunk_dict[chunk_id][\"chunk\"]\n",
        "            }\n",
        "            for chunk_id in rrf_scores.keys()\n",
        "            if chunk_id in chunk_dict\n",
        "        ][:nr_rrf_docs]\n",
        "        #print(f\"length top_documents: {len(top_documents)}\")\n",
        "    else:\n",
        "        #Skip RRF and return top nr_docs documents directly\n",
        "        top_documents = [\n",
        "            {\"title\": result[\"title\"], \"chunk\": result[\"chunk\"]}\n",
        "            for subquery_results in search_results.values()\n",
        "            for result in subquery_results][:nr_docs]\n",
        "\n",
        "\n",
        "    #Extract chunks of the top_documents to export to df and use for evaluation\n",
        "    top_chunks = [doc[\"chunk\"] for doc in top_documents]\n",
        "    #print(f\"Top chunks: {top_chunks}\")\n",
        "    \n",
        "    #Format documents for LLM readability\n",
        "    sources_formatted_complex = \"=================\\n\".join(\n",
        "        [f'TITLE: {doc[\"title\"]}, CONTENT: {doc[\"chunk\"]}' for doc in top_documents]\n",
        "    )\n",
        "\n",
        "    #Generate LLM response with the relevant documents\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": grounded_prompt_expansion.format(\n",
        "                    original_query=query,\n",
        "                    transformed_query=query_expansion,\n",
        "                    sources=sources_formatted_complex,\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        model=deployment_name\n",
        "    )\n",
        "    return response.choices[0].message.content, top_chunks\n",
        "\n",
        "\n",
        "def search_multiple_with_optional_ranking(query, rerank, search_client, deployment_name):\n",
        "    query_multiple_response = client.beta.chat.completions.parse(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": MULTIPLE_PROMPT.format(query=query, max_num_queries_multiple=max_num_queries_multiple)\n",
        "            }\n",
        "        ],\n",
        "        model=deployment_name,\n",
        "        response_format=MultipleQueryEvent\n",
        "    )\n",
        "\n",
        "    #Contains list of subqueries decomposed from the original user query\n",
        "    query_decomposition = json.loads(query_multiple_response.choices[0].message.content)[\"queries\"]\n",
        "    subquery_results = []\n",
        "    \n",
        "    #Stores all top chunks from all subqueries to put into the dataframe\n",
        "    top_chunks = []\n",
        "\n",
        "    #Perform vector search and reranking separately for each subquery\n",
        "    #Store all results for all subqueries with their top nr_docs reranked documents in subquery_results\n",
        "    for sub_query in query_decomposition:\n",
        "        vector_query = VectorizableTextQuery(text=sub_query, k_nearest_neighbors=nr_search, fields=\"text_vector\")\n",
        "        search_results = list(search_client.search(\n",
        "            search_text=sub_query,\n",
        "            vector_queries=[vector_query],\n",
        "            select=[\"title\", \"chunk\"],\n",
        "            top=nr_search,\n",
        "        ))\n",
        "\n",
        "        if rerank:\n",
        "            reranker_input = [[sub_query, document['chunk']] for document in search_results]\n",
        "            scores = reranker.compute_score(reranker_input, normalize=True)\n",
        "            ranked_documents = [\n",
        "                {\n",
        "                    \"title\": document[\"title\"],\n",
        "                    \"chunk\": document[\"chunk\"],\n",
        "                    \"score\": score,\n",
        "                }\n",
        "                for document, score in zip(search_results, scores)\n",
        "            ]\n",
        "            sorted_documents = sorted(ranked_documents, key=itemgetter(\"score\"), reverse=True)[:nr_docs]\n",
        "        else:\n",
        "            #Skip reranking and return top nr_docs documents directly\n",
        "            sorted_documents = search_results[:nr_docs]\n",
        "        \n",
        "        #Store chunks\n",
        "        top_chunks.extend([doc[\"chunk\"] for doc in sorted_documents])\n",
        "\n",
        "        subquery_results.append({\n",
        "            \"sub_query\": sub_query,\n",
        "            \"documents\": sorted_documents\n",
        "        })\n",
        "\n",
        "        #Format each sub_query separately with its sources\n",
        "    sources_formatted_multiple = \"\\n\\n\".join(\n",
        "        f\"SUB-QUERY: {result['sub_query']}\\n=================\\n\" +\n",
        "        \"\\n=================\\n\".join(\n",
        "            f\"TITLE: {doc['title']}, CONTENT: {doc['chunk']}\"\n",
        "            for doc in result[\"documents\"]\n",
        "        )\n",
        "        for result in subquery_results\n",
        "    )\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": grounded_prompt_decomposition.format(\n",
        "                    original_query=query,\n",
        "                    transformed_query=query_decomposition,\n",
        "                    sources=sources_formatted_multiple,\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        model=deployment_name\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content, top_chunks"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1752012209046
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_non_agentic(query, rerank, search_client, deployment_name):\n",
        "    '''\n",
        "    Execute non agentic retrieval; direct embedding of the user query\n",
        "    '''\n",
        "    #Configure vectorized query\n",
        "    vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=nr_search, fields=\"text_vector\")\n",
        "\n",
        "    #Perform vector search directly on the query\n",
        "    #Search_results is an iterator object containing the relevant documents\n",
        "    #Convert result to list to prevent exhausting the iterator\n",
        "    search_results = list(search_client.search(\n",
        "        search_text=query,\n",
        "        vector_queries=[vector_query],\n",
        "        select=[\"title\", \"chunk\"],\n",
        "        top=nr_search,\n",
        "    ))\n",
        "\n",
        "    top_documents = search_results[:nr_docs]\n",
        "\n",
        "    #Extract chunks of the top_documents to export to df and use for evaluation\n",
        "    top_chunks = [doc[\"chunk\"] for doc in top_documents]\n",
        "\n",
        "    #Format documents for LLM readability\n",
        "    sources_formatted = \"=================\\n\".join(\n",
        "        [f'TITLE: {doc[\"title\"]}, CONTENT: {doc[\"chunk\"]}' for doc in top_documents]\n",
        "    )\n",
        "\n",
        "    #Generate LLM response using the relevant documents\n",
        "    response = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": grounded_prompt_non_agentic.format(\n",
        "                    original_query=query,\n",
        "                    sources=sources_formatted,\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        model=deployment_name\n",
        "    )\n",
        "    return response.choices[0].message.content, top_chunks"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1752012213668
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, search_client, deployment_name, use_agentic=True, use_reranking=True):\n",
        "    \"\"\"\n",
        "    Retrieves documents based on the query, with configurable options.\n",
        "\n",
        "    Args:\n",
        "    - query (str): The user's query.\n",
        "    - use_agentic (bool): Whether to use agentic retrieval (classification and query transformation).\n",
        "    - use_reranking (bool): Whether to use reranking or reciprocal rank fusion in agentic retrieval.\n",
        "\n",
        "    Returns:\n",
        "    str: The final response with the relevant documents.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    if not use_agentic:\n",
        "        #Direct embedding search without classification or query transformation\n",
        "        response, top_chunks = search_non_agentic(query, rerank=use_reranking, search_client=search_client, deployment_name=deployment_name)\n",
        "        #In case search_conversational is called, return response with empty chunk list for the dataframe\n",
        "        #print(\"No classification in non agentic retrieval\")\n",
        "        return response, top_chunks\n",
        "\n",
        "    #If agentic retrieval is enabled, proceed with classification\n",
        "    classification = classify_query(query, deployment_name)\n",
        "    #Based on classification and reranking configuration, perform the corresponding query transformation and reranking method\n",
        "    if classification == 1:\n",
        "        response = search_conversational(query, deployment_name)\n",
        "        response, top_chunks = response, []\n",
        "    elif classification == 2:\n",
        "        response, top_chunks = (\n",
        "            search_simple_with_optional_ranking(query, rerank=use_reranking, search_client=search_client, deployment_name=deployment_name) \n",
        "            if use_reranking \n",
        "            else search_simple_with_optional_ranking(query, rerank=False, search_client=search_client, deployment_name=deployment_name)\n",
        "        )\n",
        "    elif classification == 3:\n",
        "        response, top_chunks = (\n",
        "            search_complex_with_optional_ranking(query, rerank=use_reranking, search_client=search_client, deployment_name=deployment_name)\n",
        "            if use_reranking \n",
        "            else search_complex_with_optional_ranking(query, False, search_client=search_client, deployment_name=deployment_name)\n",
        "        )\n",
        "    elif classification == 4:\n",
        "        response, top_chunks = (\n",
        "            search_multiple_with_optional_ranking(query, rerank=use_reranking, search_client=search_client, deployment_name=deployment_name) \n",
        "            if use_reranking \n",
        "            else search_multiple_with_optional_ranking(query, False, search_client=search_client, deployment_name=deployment_name)\n",
        "        )\n",
        "    else:\n",
        "        print(\"ERROR classification is not in range 1:4\")\n",
        "    return response, top_chunks"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1752013281487
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_all_combinations2(df_questions, query_column, index_names, methods, llm_deployments, num_iterations=1):\n",
        "    \"\"\"\n",
        "    Evaluates all combinations of search indexes and retrieval methods, and saves responses in a dataframe.\n",
        "\n",
        "    Arguments:\n",
        "    - df (Dataframe): Pandas dataframe that contains the user queries (questions).\n",
        "    - query_column (str): The column name in the df that contains the queries.\n",
        "    - index_names (list): List of index names from Azure AI Search to evaluate.\n",
        "    - methods (list): List of retrieval methods to evaluate. Each method is a tuple: (method_name, use_agentic, rerank).\n",
        "    - llm_deployments (list): List of LLM deployment names used for classification, transformation, and generation.\n",
        "    - num_iterations (int): Number of times to run each query for each model configuration.\n",
        "\n",
        "    Returns:\n",
        "    Dataframe: The updated dataframe with responses for each combination of indexes and methods.\n",
        "    \"\"\"\n",
        "\n",
        "    #Temp storage for responses, time, and document chunks to put into dataframe\n",
        "    all_results = []\n",
        "\n",
        "    for deployment_name in llm_deployments:\n",
        "        for idx_name in index_names:  \n",
        "            #Iterate over all index names\n",
        "\n",
        "            #Recreate searchclient each time a new index is used\n",
        "            search_client = SearchClient(\n",
        "                endpoint=service_endpoint,\n",
        "                index_name=idx_name,\n",
        "                credential=AzureKeyCredential(key)\n",
        "            )\n",
        "        \n",
        "            for method_name, use_agentic, use_reranking in methods:  \n",
        "                \n",
        "                #Iterate over all methods\n",
        "                try:\n",
        "                    df_results = pd.read_pickle(f\"Backups/backup_{deployment_name}_{idx_name}_{method_name}\")\n",
        "                    print(f\"Succesfully loaded: {idx_name} | Method: {method_name} | LLM: {deployment_name}\")\n",
        "\n",
        "                except Exception as err:\n",
        "                    print(err)\n",
        "                    print(f\"Evaluating Index: {idx_name} | Method: {method_name} | LLM: {deployment_name}\")                    \n",
        "\n",
        "                    results = []\n",
        "                    # For each query, generate responses for each iteration, and for each method configuration\n",
        "                    for query in df_questions[query_column]:\n",
        "                        for iteration in range(num_iterations):\n",
        "                            start_time = time.time()\n",
        "\n",
        "                            response, top_chunks = retrieve(query, search_client=search_client, use_agentic=use_agentic, use_reranking=use_reranking, deployment_name=deployment_name)\n",
        "\n",
        "                            end_time = time.time()\n",
        "                            \n",
        "                            processing_time = end_time - start_time\n",
        "                            \n",
        "                            result = {\n",
        "                                \"deployment_name\":deployment_name,\n",
        "                                \"idx_name\":idx_name,\n",
        "                                \"method_name\":method_name,\n",
        "                                \"use_agentic\":use_agentic,\n",
        "                                \"use_reranking\":methods,\n",
        "                                \"query\":query,\n",
        "                                \"iteration\":iteration,\n",
        "                                \"response\":response,\n",
        "                                \"time\":processing_time,\n",
        "                                \"results\":top_chunks                            \n",
        "                            }\n",
        "                            results.append(result)\n",
        "\n",
        "                            #Sleep to prevent exceeding rate limits\n",
        "                            time.sleep(4)\n",
        "                \n",
        "                    df_results = pd.DataFrame(results)\n",
        "                    df_results.to_pickle(f\"Backups/backup_{deployment_name}_{idx_name}_{method_name}\")\n",
        "                all_results.append(df_results)\n",
        "\n",
        "    #Update the dataframe with the results from all queries\n",
        "    df = pd.concat(all_results)\n",
        "    df.to_pickle(\"full_dataframe_pickle\")\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1752015172525
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('Vragen HAFIR.xlsx')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#df_llm_response = evaluate_all_combinations2(df_vragen, \"Vraag\", all_index_names, selected_methods, deployment_names, 3)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1752041402919
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run inference pipeline in batches"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def process_in_batches(df, query_column, batch_size, index_names, methods, llm_deployments, num_iterations=3):\n",
        "    \"\"\"\n",
        "    Processes the input dataframe in smaller batches and save results incrementally.\n",
        "    \n",
        "    \"\"\"\n",
        "    for start_idx in range(0, len(df), batch_size):\n",
        "        #Extract a single batch of queries\n",
        "        batch_df = df.iloc[start_idx:start_idx + batch_size].copy()\n",
        "\n",
        "        print(f\"Processing batch {start_idx // batch_size + 1}...\")\n",
        "        try:\n",
        "            #Process the batch through evaluate_all_combinations\n",
        "            updated_df = evaluate_all_combinations2(\n",
        "                batch_df,\n",
        "                query_column=query_column,\n",
        "                index_names=index_names,\n",
        "                methods=methods,\n",
        "                llm_deployments=llm_deployments,\n",
        "                num_iterations=num_iterations\n",
        "            )\n",
        "            \n",
        "            #save the batch results into a separate file\n",
        "            batch_file_name = f\"results_batch_{start_idx}.csv\"\n",
        "            updated_df.to_csv(batch_file_name, index=False)\n",
        "            print(f\"Batch {start_idx // batch_size + 1} saved successfully as '{batch_file_name}'!\")\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred in batch {start_idx // batch_size + 1}: {e}\")\n"
      ],
      "outputs": [],
      "execution_count": 213,
      "metadata": {
        "gather": {
          "logged": 1751930033168
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_in_batches(df, \"Vraag\", batch_size=2, index_names= all_index_names, methods = selected_methods, llm_deployments=deployment_names, num_iterations=3,)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1751944813149
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Small df For testing the pipeline:\n",
        "#Create a temporary dataframe containing only the first question\n",
        "df_first_two = df.tail(1).copy()\n",
        "\n",
        "#Execute the retrieval for only the first question\n",
        "#updated_df = retrieve_multiple_indexes_and_methods(df_first_two, \"Vraag\", selected_methods, all_index_names)\n",
        "updated_df = evaluate_all_combinations2(df_first_two, \"Vraag\", all_index_names, selected_methods, deployment_names, 3)\n",
        "\n",
        "updated_df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Evaluating Index: idx-exhaustiveknn-custom-vectorizer-384 | Method: Non-agentic retrieval | LLM: gpt-4o\nEvaluating Index: idx-exhaustiveknn-custom-vectorizer-384 | Method: Agentic retrieval without reranking | LLM: gpt-4o\nEvaluating Index: idx-exhaustiveknn-custom-vectorizer-384 | Method: Agentic retrieval with reranking | LLM: gpt-4o\nEvaluating Index: idx-hnsw-openai-vectorizer-text-embedding-3-large | Method: Non-agentic retrieval | LLM: gpt-4o\nEvaluating Index: idx-hnsw-openai-vectorizer-text-embedding-3-large | Method: Agentic retrieval without reranking | LLM: gpt-4o\nEvaluating Index: idx-hnsw-openai-vectorizer-text-embedding-3-large | Method: Agentic retrieval with reranking | LLM: gpt-4o\n"
        }
      ],
      "execution_count": 177,
      "metadata": {
        "gather": {
          "logged": 1751927646343
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.10 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
